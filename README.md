# Agentic Pipeline for High-Accuracy Speaker-Separated Subtitle Generation

This project implements an agentic pipeline in a Google Colab environment to generate accurate, speaker-separated subtitles for video clips. It demonstrates a multi-agent workflow where specialized AI models collaborate to perform complex media processing tasks, resulting in high-quality, verified subtitle output.

Video Link->https://drive.google.com/drive/folders/1v3cJuzTmsg_XXz1HyHZngvbeDoDiSOkd?usp=sharing

## Project Objective

The main objective of this project is to create a robust and flexible pipeline for generating subtitles that are not only accurate in transcription but also correctly attribute dialogue to different speakers. This is achieved by integrating several state-of-the-art AI models in a sequential workflow.

## Pipeline Stages

The pipeline is structured into the following key stages:

1.  **Initialize Libraries and Download Dependencies**: Ensures all necessary Python libraries (`yt-dlp`, `ffmpeg-python`, `openai-whisper`, `pyannote.audio`, `google-generativeai`, `sarvamai`, `pydub`) are installed.
2.  **Download Video and Extract Audio**: Handles input video (YouTube URL or local file), downloads it using `yt-dlp` (if online), and extracts the audio into a standardized WAV format (`assignment_audio.wav`) using `pydub`.
3.  **Transcribe Audio with Whisper**: Processes the extracted audio using the `openai-whisper` model to generate a detailed transcription with word-level timestamps.
4.  **Perform Speaker Diarization with Pyannote**: Utilizes the `pyannote.audio` library to perform speaker diarization, identifying who is speaking and when, segmenting audio into speaker turns.
5.  **Generate SRT from Transcription and Diarization**: A custom function aligns the word-level timestamps and text from Whisper with the speaker turns from Pyannote to create a preliminary SRT file (`srt_output`).
6.  **Correct SRT with Gemini**: The draft SRT, initial diarization timeline, and the original audio are passed to a Refinement Agent powered by a large multimodal model (Gemini). This agent performs a holistic review, correcting errors in timing, speaker assignment, and transcription to produce the final, polished SRT file (`output.srt`).

## Quality Check

7.  **Burn Subtitles into Video**: Using FFmpeg, the corrected SRT file is embedded into the original video file (`final_video_with_subtitles.mp4`) for visual review.
8.  **Quality Check the Subtitles with Gemini**: Gemini is used as a final QA step to provide a confidence score and qualitative feedback on the final corrected SRT (`output.srt`) by comparing it against the original audio. A report detailing the accuracy is saved as `quality_report.csv`.

## Why a Custom Pipeline (Not a Framework)?

This project intentionally uses simple Python functions and sequential execution instead of a dedicated agentic framework like LangChain or LlamaIndex. This approach was chosen to:

*   **Demonstrate Understanding of Fundamentals**: Showcases the ability to integrate underlying technologies directly.
*   **Offer Flexibility and Customization**: Allows for fine-grained control over each step and easier experimentation.
*   **Reduce Overhead**: Avoids the complexity and setup required by larger frameworks in a Colab environment.
*   **Ensure Clear Data Flow and Debugging**: Makes the process transparent and easier to troubleshoot.
*   **Focus on Core Logic**: Highlights the problem-solving approach and implementation details.
*   **Promote Adaptability**: Makes the core logic portable to different environments.

## Setup and Usage

1.  Open the provided Google Colab notebook.
2.  Ensure you have the necessary API keys for Google Gemini and Hugging Face (for Pyannote) stored securely in Colab's Secrets Manager (`GOOGLE_API_KEY` and `HF_TOKEN`). If using the optional SarvamAI stage, add your key as `SARVAMAI`.
3.  Run the cells sequentially.
4.  When prompted, provide a YouTube URL, a local video file path, or upload a video file.
5.  The notebook will execute the pipeline stages, producing the final `output.srt` file, a video with burned-in subtitles (`final_video_with_subtitles.mp4`), and a quality report (`quality_report.csv`).

## Files

*   `notebook_name.ipynb`: The main Colab notebook containing the pipeline code. (Replace `notebook_name.ipynb` with the actual name of your notebook file)
*   `assignment_video.mp4`: (Generated) The downloaded or processed video file.
*   `assignment_audio.wav`: (Generated) The extracted audio file.
*   `output.srt`: (Generated) The final, corrected SRT subtitle file.
*   `final_video_with_subtitles.mp4`: (Generated) The video with subtitles burned in.
*   `quality_report.csv`: (Generated) The quality report generated by the QA Agent.
*   `sarika_audio.wav`: (Generated - Optional) Extracted audio for Sarika.

