{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrDXp3WItEEwBq3ioV4A/i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carnage203/Video-Subtitle-Generator-Agent/blob/main/agentic_pipeline_Soham_Mandal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Documentation: An Agentic Pipeline for High-Accuracy Subtitle Generation\n",
        "\n",
        "#Objective\n",
        "This document outlines an agentic pipeline developed to generate accurate, speaker-separated subtitles for video clips. The pipeline demonstrates a multi-agent workflow where specialized AI models collaborate to perform complex media processing tasks, culminating in a high-quality, verified output.\n",
        "\n",
        "\n",
        "## Pipeline Stages:\n",
        "\n",
        "The pipeline is structured into the following stages:\n",
        "\n",
        "1.  **Initialize Libraries and Download Dependencies**: This initial stage ensures that all necessary Python libraries (`yt-dlp`, `ffmpeg-python`, `openai-whisper`, `pyannote.audio`, `google-generativeai`, `sarvamai`, `pydub`) are installed in the environment. This is a foundational step to ensure the subsequent stages can execute without dependency errors.\n",
        "\n",
        "2.  **Download Video and Extract Audio**: This stage handles the input video. It can accept a YouTube URL or a local video file. Using `yt-dlp`, the video is downloaded, and then `pydub` is used to extract the audio stream and convert it into a standardized WAV format (`assignment_audio.wav`). This ensures a consistent audio input for the downstream tasks.\n",
        "\n",
        "3.  **Transcribe Audio with Whisper**: The extracted audio is processed by the Whisper model (`openai-whisper`) to generate a complete transcription. This stage provides the textual content of the spoken dialogue along with word-level timestamps, which are crucial for aligning with speaker information later.\n",
        "\n",
        "4.  **Perform Speaker Diarization with Pyannote**: In parallel with transcription, the `pyannote.audio` library is used to perform speaker diarization. This process identifies *who* is speaking and *when*. It segments the audio into speaker turns, labeling each segment with a speaker identifier (e.g., SPEAKER_00, SPEAKER_01). This output is essential for separating dialogue by speaker in the subtitles.\n",
        "\n",
        "5.  **Generate SRT from Transcription and Diarization**: This is a critical integration stage. A custom function (`generate_srt_from_results`) takes the word-level timestamps and text from the Whisper transcription and the speaker turns from the Pyannote diarization. It aligns these two pieces of information to create a preliminary SRT file (`srt_output`) where each subtitle entry includes the text spoken, the corresponding timestamps, and the identified speaker.\n",
        "\n",
        "6.  **Correct SRT with Gemini** : To achieve state-of-the-art accuracy, the draft SRT, the initial diarization timeline, and the original audio are passed to the Refinement Agent, powered by a large multimodal model (Gemini). This agent performs a holistic review, correcting any residual errors in timing, speaker assignment, or transcription, and generates the final, polished SRT file (`output.srt`).\n",
        "\n",
        "#Quality Check\n",
        "\n",
        "7.  **Burn Subtitles into Video**: Using FFmpeg, the corrected SRT file is embedded directly into the original video file. This creates a new video file (`final_video_with_subtitles.mp4`) with the subtitles permanently displayed, providing a visual output for review. (Human in loop)\n",
        "\n",
        "8.  **Quality Check the Subtitles with Gemini**: As a final quality assurance step, Gemini is used again to provide a confidence score and qualitative feedback on the final corrected SRT by comparing it against the original audio. This generates a report (`quality_report.csv`) detailing the accuracy of the transcription, speaker labels, and timestamps for each subtitle chunk.\n",
        "\n",
        "9.  **Extract Initial Audio for Sarika (Optional)**: This stage demonstrates the potential to integrate other ASR services. A short segment of the audio is extracted to be sent to the Sarika model.\n",
        "\n",
        "10. **Transcribe with Sarika (Optional)**: Using the SarvamAI client and the Saarika model, the extracted audio segment is transcribed. This showcases how different transcription services can be incorporated into the pipeline.\n",
        "\n",
        "\n",
        "## Justification for Not Using a Specific Agentic Framework:\n",
        "\n",
        "While several excellent agentic frameworks exist (e.g., LangChain, LlamaIndex), this notebook intentionally utilizes simple Python functions and sequential execution within a Colab environment. This deliberate choice was made based on the following considerations, particularly relevant for demonstrating core engineering and problem-solving skills in an internship context:\n",
        "\n",
        "*   **Demonstrated Understanding of Fundamentals:** Building the pipeline from individual components showcases a deeper understanding of the underlying technologies (ASR, diarization, LLM integration) and how they interact. It highlights the ability to integrate disparate tools to solve a complex problem, rather than relying on a framework's pre-built abstractions.\n",
        "*   **Flexibility and Customization:** The modular nature of this approach allows for fine-grained control over each step. This is crucial for iterating on the pipeline, experimenting with different models or parameters, and addressing specific edge cases that might be difficult to handle within a more opinionated framework. For an internship, this demonstrates adaptability and a problem-solving mindset.\n",
        "*   **Efficiency and Reduced Overhead:** For a task of this scope and within the Colab environment, introducing a full-fledged agentic framework could add unnecessary complexity and overhead in terms of setup, learning curve, and potential debugging within the framework itself. A simpler, direct implementation is more efficient for demonstrating the core functionality.\n",
        "*   **Clear Data Flow and Debugging:** The sequential execution and explicit function calls make the data flow between stages transparent. This simplifies debugging and understanding where issues might arise in the pipeline, a valuable skill in any development role.\n",
        "*   **Focus on Core Logic:** By avoiding framework-specific boilerplate, the code directly focuses on the core logic of integrating the AI models and processing the data. This highlights the problem-solving approach and the implementation details of the subtitling pipeline itself.\n",
        "*   **Adaptability to Different Environments:** While demonstrated in Colab, the core logic and the use of standard libraries make this pipeline relatively portable and adaptable to different deployment environments, showcasing an understanding of practical software development considerations beyond a specific framework.\n",
        "\n",
        "##Conclusion and Future Work\n",
        "The implemented pipeline successfully meets the assignment's objectives, demonstrating an effective multi-agent workflow that produces high-quality, diarized subtitles.\n",
        "\n",
        "##Potential improvements and future work include:\n",
        "\n",
        "Alternative Architectures: Implementing a \"diarize-then-transcribe\" pipeline (where audio is first split into speaker chunks and then transcribed) as an alternative for comparison.\n",
        "\n",
        "Automated Error Flagging: Enhancing the QA Agent to automatically flag subtitles with low confidence scores for targeted human review.\n",
        "\n",
        "Model Benchmarking: Integrating other models (like the optional Sarika stage) more formally to benchmark their performance on key metrics like Word Error Rate (WER) against the baseline."
      ],
      "metadata": {
        "id": "dYj0BVTFBgFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigned Task:\n"
      ],
      "metadata": {
        "id": "ms7laouXyv_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading dependencies"
      ],
      "metadata": {
        "id": "Ow_QRKFKy7rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "48onAOgnpHIf",
        "outputId": "58cf8e75-c81f-4ee8-99f6-5c7b1e7480a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.7.21)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: pyannote.audio in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.33.4)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.5.2)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (5.1.3)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.8.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (13.9.4)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (3.0.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.13.1)\n",
            "Requirement already satisfied: speechbrain>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (1.0.3)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.4)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (0.12.0)\n",
            "Requirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio) (1.7.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (1.1.5)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote.audio) (0.14.3)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote.audio) (2.5.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.6.1)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.0)\n",
            "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.13.1)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio) (5.29.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.11/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.18.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.7.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.20.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.2.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install ffmpeg-python\n",
        "!pip install openai-whisper\n",
        "!pip install pyannote.audio\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "sarvamai_key = userdata.get('SARVAMAI')\n",
        "print(\"API keys loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGnaIE4g20Zi",
        "outputId": "81cea0b9-f34c-4c9a-c601-f1beb6b05ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall approach is to create a flexible and robust data preparation pipeline. User provide the video, either from an online source like YouTube or a local file on the user's machine, and convert it into a standardized, high-quality WAV audio file. This audio file (assignment_audio.wav) serves as the essential input for the subsequent, more complex stages of the project, such as speaker diarization and transcription.\n",
        "\n",
        "Can handle both YouTube link and Video upload\n",
        "\n"
      ],
      "metadata": {
        "id": "JFzTVYDB0L02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "\n",
        "def download_video_and_extract_audio(video_url, output_video_path, output_audio_path):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': output_video_path,\n",
        "        'quiet': False,\n",
        "    }\n",
        "    try:\n",
        "        print(f\"ðŸ“¥ Starting video download for: {video_url}\")\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([video_url])\n",
        "        if not os.path.exists(output_video_path):\n",
        "             raise FileNotFoundError(f\"Video download failed, file not found at: {output_video_path}\")\n",
        "        print(f\"âœ… Video successfully downloaded to: {output_video_path}\")\n",
        "        print(f\"\\nðŸŽ§ Extracting audio from '{output_video_path}'...\")\n",
        "        video_segment = AudioSegment.from_file(output_video_path, format=\"mp4\")\n",
        "        video_segment.export(output_audio_path, format=\"wav\")\n",
        "        print(f\"âœ… Audio successfully extracted to: {output_audio_path}\")\n",
        "        return output_video_path, output_audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_local_video(video_path, output_audio_path=\"extracted_audio.wav\"):\n",
        "    try:\n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"File not found at: {video_path}\")\n",
        "        print(f\"\\nðŸŽ§ Extracting audio from '{video_path}'...\")\n",
        "        video_segment = AudioSegment.from_file(video_path)\n",
        "        video_segment.export(output_audio_path, format=\"wav\")\n",
        "        print(f\"âœ… Audio successfully extracted to: {output_audio_path}\")\n",
        "        return video_path, output_audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    user_input = input(\"Enter a YouTube URL, a local file path, or leave blank to upload: \")\n",
        "    VIDEO_OUTPUT_PATH = \"assignment_video.mp4\"\n",
        "    AUDIO_OUTPUT_PATH = \"assignment_audio.wav\"\n",
        "    result = None\n",
        "\n",
        "    if user_input.strip().startswith('http'):\n",
        "        result = download_video_and_extract_audio(user_input, VIDEO_OUTPUT_PATH, AUDIO_OUTPUT_PATH)\n",
        "    elif user_input.strip() and os.path.exists(user_input.strip()):\n",
        "        result = process_local_video(user_input, AUDIO_OUTPUT_PATH)\n",
        "    elif not user_input.strip():\n",
        "        print(\"Please select a video file to upload.\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            uploaded_filename = next(iter(uploaded))\n",
        "            print(f\"âœ… File '{uploaded_filename}' uploaded successfully.\")\n",
        "            result = process_local_video(uploaded_filename, AUDIO_OUTPUT_PATH)\n",
        "        else:\n",
        "            print(\"âŒ No file was uploaded.\")\n",
        "    else:\n",
        "        print(f\"âŒ Input '{user_input}' is not a valid URL or an existing file path.\")\n",
        "\n",
        "    if result:\n",
        "        video_file, audio_file = result\n",
        "        video_exists = os.path.exists(video_file)\n",
        "        audio_exists = os.path.exists(audio_file)\n",
        "        print(\"\\n--- Verification ---\")\n",
        "        print(f\"Video file '{video_file}' exists: {video_exists}\")\n",
        "        print(f\"Audio file '{audio_file}' exists: {audio_exists}\")\n",
        "        if video_exists and audio_exists:\n",
        "            print(\"\\nâœ… Both files are ready for the next step.\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Verification failed. One or both files were not created.\")\n",
        "    else:\n",
        "        print(\"\\nâŒ Process failed.\")\n",
        "\n",
        "#https://www.youtube.com/watch?v=zYJKq17GpEc -> Reference video given"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgJ8Hkbw1bVm",
        "outputId": "e970368f-898f-412d-fcfb-0c7db46b0b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a YouTube URL, a local file path, or leave blank to upload: https://www.youtube.com/watch?v=zYJKq17GpEc\n",
            "ðŸ“¥ Starting video download for: https://www.youtube.com/watch?v=zYJKq17GpEc\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zYJKq17GpEc\n",
            "[youtube] zYJKq17GpEc: Downloading webpage\n",
            "[youtube] zYJKq17GpEc: Downloading tv client config\n",
            "[youtube] zYJKq17GpEc: Downloading tv player API JSON\n",
            "[youtube] zYJKq17GpEc: Downloading ios player API JSON\n",
            "[youtube] zYJKq17GpEc: Downloading m3u8 information\n",
            "[info] Testing format 614\n",
            "[info] zYJKq17GpEc: Downloading 1 format(s): 614+328\n",
            "[download] assignment_video.mp4 has already been downloaded\n",
            "âœ… Video successfully downloaded to: assignment_video.mp4\n",
            "\n",
            "ðŸŽ§ Extracting audio from 'assignment_video.mp4'...\n",
            "âœ… Audio successfully extracted to: assignment_audio.wav\n",
            "\n",
            "--- Verification ---\n",
            "Video file 'assignment_video.mp4' exists: True\n",
            "Audio file 'assignment_audio.wav' exists: True\n",
            "\n",
            "âœ… Both files are ready for the next step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Whisper for the entire transcription from the extracted audio"
      ],
      "metadata": {
        "id": "GjuL8mHH2fs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import pprint\n",
        "model = whisper.load_model(\"large\")\n",
        "whisper_result = model.transcribe(\n",
        "    \"/content/assignment_audio.wav\",\n",
        "    word_timestamps=True\n",
        ")\n",
        "\n",
        "pprint.pprint(whisper_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SMS9hEnmvAH7",
        "outputId": "3a5f4d17-0dd2-4061-a3a8-382734a0384b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Pyannote for the speaker diarization from the extracted audio"
      ],
      "metadata": {
        "id": "flCZQz133ysX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "\n",
        "hf_token = hf_token\n",
        "\n",
        "print(\"Loading diarization pipeline...\")\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "  \"pyannote/speaker-diarization-3.1\",\n",
        "  use_auth_token=hf_token)\n",
        "print(\"âœ… Pipeline loaded.\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Moving pipeline to GPU...\")\n",
        "    pipeline.to(torch.device(\"cuda\"))\n",
        "    print(\"âœ… Pipeline on GPU.\")\n",
        "\n",
        "# ---\n",
        "\n",
        "\n",
        "audio_path = \"/content/assignment_audio.wav\"\n",
        "print(f\"Running diarization on {audio_path}...\")\n",
        "diarization_result = pipeline(audio_path)\n",
        "print(\"âœ… Diarization complete.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Diarization Result ---\")\n",
        "print(diarization_result)\n",
        "\n",
        "\n",
        "print(\"\\n--- Speaker Turns ---\")\n",
        "for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
        "    print(f\"Start: {turn.start:.2f}s | End: {turn.end:.2f}s | Speaker: {speaker}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz6gornQwdg6",
        "outputId": "306e5a21-813e-46b3-f13f-2a66c2915a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading diarization pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pipeline loaded.\n",
            "Running diarization on /content/assignment_audio.wav...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  std = sequences.std(dim=-1, correction=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Diarization complete.\n",
            "\n",
            "--- Diarization Result ---\n",
            "[ 00:00:01.026 -->  00:00:01.954] A SPEAKER_00\n",
            "[ 00:00:02.933 -->  00:00:03.507] B SPEAKER_00\n",
            "[ 00:00:05.228 -->  00:00:06.780] C SPEAKER_00\n",
            "[ 00:00:07.523 -->  00:00:16.045] D SPEAKER_00\n",
            "[ 00:00:08.654 -->  00:00:09.210] E SPEAKER_01\n",
            "[ 00:00:17.108 -->  00:00:18.779] F SPEAKER_00\n",
            "[ 00:00:19.099 -->  00:00:35.097] G SPEAKER_00\n",
            "[ 00:00:20.264 -->  00:00:21.057] H SPEAKER_01\n",
            "[ 00:00:21.107 -->  00:00:22.255] I SPEAKER_01\n",
            "[ 00:00:23.132 -->  00:00:25.360] J SPEAKER_01\n",
            "[ 00:00:26.862 -->  00:00:29.562] K SPEAKER_01\n",
            "[ 00:00:30.659 -->  00:00:31.148] L SPEAKER_01\n",
            "[ 00:00:35.873 -->  00:00:36.953] M SPEAKER_00\n",
            "[ 00:00:37.392 -->  00:01:00.240] N SPEAKER_00\n",
            "[ 00:00:42.134 -->  00:00:43.872] O SPEAKER_01\n",
            "[ 00:00:45.458 -->  00:00:47.888] P SPEAKER_01\n",
            "[ 00:00:55.769 -->  00:00:57.017] Q SPEAKER_01\n",
            "[ 00:01:00.797 -->  00:01:02.130] R SPEAKER_00\n",
            "[ 00:01:02.367 -->  00:01:07.564] S SPEAKER_00\n",
            "[ 00:01:08.627 -->  00:01:10.534] T SPEAKER_00\n",
            "[ 00:01:11.159 -->  00:01:13.032] U SPEAKER_00\n",
            "[ 00:01:13.538 -->  00:01:14.702] V SPEAKER_00\n",
            "[ 00:01:15.968 -->  00:01:16.862] W SPEAKER_00\n",
            "[ 00:01:17.014 -->  00:01:20.929] X SPEAKER_00\n",
            "[ 00:01:22.414 -->  00:01:23.325] Y SPEAKER_00\n",
            "\n",
            "--- Speaker Turns ---\n",
            "Start: 1.03s | End: 1.95s | Speaker: SPEAKER_00\n",
            "Start: 2.93s | End: 3.51s | Speaker: SPEAKER_00\n",
            "Start: 5.23s | End: 6.78s | Speaker: SPEAKER_00\n",
            "Start: 7.52s | End: 16.05s | Speaker: SPEAKER_00\n",
            "Start: 8.65s | End: 9.21s | Speaker: SPEAKER_01\n",
            "Start: 17.11s | End: 18.78s | Speaker: SPEAKER_00\n",
            "Start: 19.10s | End: 35.10s | Speaker: SPEAKER_00\n",
            "Start: 20.26s | End: 21.06s | Speaker: SPEAKER_01\n",
            "Start: 21.11s | End: 22.26s | Speaker: SPEAKER_01\n",
            "Start: 23.13s | End: 25.36s | Speaker: SPEAKER_01\n",
            "Start: 26.86s | End: 29.56s | Speaker: SPEAKER_01\n",
            "Start: 30.66s | End: 31.15s | Speaker: SPEAKER_01\n",
            "Start: 35.87s | End: 36.95s | Speaker: SPEAKER_00\n",
            "Start: 37.39s | End: 60.24s | Speaker: SPEAKER_00\n",
            "Start: 42.13s | End: 43.87s | Speaker: SPEAKER_01\n",
            "Start: 45.46s | End: 47.89s | Speaker: SPEAKER_01\n",
            "Start: 55.77s | End: 57.02s | Speaker: SPEAKER_01\n",
            "Start: 60.80s | End: 62.13s | Speaker: SPEAKER_00\n",
            "Start: 62.37s | End: 67.56s | Speaker: SPEAKER_00\n",
            "Start: 68.63s | End: 70.53s | Speaker: SPEAKER_00\n",
            "Start: 71.16s | End: 73.03s | Speaker: SPEAKER_00\n",
            "Start: 73.54s | End: 74.70s | Speaker: SPEAKER_00\n",
            "Start: 75.97s | End: 76.86s | Speaker: SPEAKER_00\n",
            "Start: 77.01s | End: 80.93s | Speaker: SPEAKER_00\n",
            "Start: 82.41s | End: 83.33s | Speaker: SPEAKER_00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Explanation:\n",
        "This  pipeline designed to merge the outputs of two separate agents: a transcription model (Whisper) and a speaker diarization model (pyannote.audio). Its goal is to produce a single, accurate, speaker-separated subtitle file in the standard SRT format.\n",
        "\n",
        "The process is broken down into two main functions:\n",
        "\n",
        "generate_srt_from_results (The Core Logic):\n",
        "\n",
        "Speaker Assignment: The function iterates through every single word provided by the transcription model. For each word, it determines which speaker was talking at that moment by checking the diarization data.\n",
        "\n",
        "Grouping into Dialogue: It then groups consecutive words spoken by the same person into a single dialogue segment. A new segment is created whenever the speaker changes.\n",
        "\n",
        "Formatting: Finally, it takes these dialogue segments and formats them into the final SRT string, using the helper function to create the required timestamps.\n",
        "\n",
        "format_timestamp_srt (A Utility):\n",
        "\n",
        "This is a helper function that takes a time in seconds (e.g., 35.482) and converts it into the strict HH:MM:SS,ms format (e.g., 00:00:35,482) that the SRT standard requires."
      ],
      "metadata": {
        "id": "EVcYLKgG4Yo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def format_timestamp_srt(seconds):\n",
        "    \"\"\"Converts seconds into the SRT timestamp format (HH:MM:SS,ms).\"\"\"\n",
        "    td = datetime.timedelta(seconds=seconds)\n",
        "    minutes, seconds = divmod(td.seconds, 60)\n",
        "    hours, minutes = divmod(minutes, 60)\n",
        "    milliseconds = td.microseconds // 1000\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
        "\n",
        "\n",
        "def generate_srt_from_results(whisper_result, diarization_result):\n",
        "    \"\"\"\n",
        "    Aligns whisper transcription with pyannote diarization, handling both\n",
        "    overlapping segments and filling VAD gaps for complete sentences.\n",
        "    \"\"\"\n",
        "    all_words = []\n",
        "    for segment in whisper_result.get('segments', []):\n",
        "        all_words.extend(segment.get('words', []))\n",
        "\n",
        "    if not all_words:\n",
        "        return \"\"\n",
        "\n",
        "    speaker_mapping = {label: f\"Speaker {i+1}\" for i, label in enumerate(diarization_result.labels())}\n",
        "\n",
        "    word_speakers = []\n",
        "    for word in all_words:\n",
        "        word_center_time = word['start'] + (word['end'] - word['start']) / 2\n",
        "\n",
        "        overlapping_speakers = []\n",
        "        for turn, _, speaker_label in diarization_result.itertracks(yield_label=True):\n",
        "            if turn.start <= word_center_time < turn.end:\n",
        "                duration = turn.end - turn.start\n",
        "                overlapping_speakers.append({\n",
        "                    'speaker': speaker_mapping.get(speaker_label),\n",
        "                    'duration': duration\n",
        "                })\n",
        "\n",
        "        found_speaker = None\n",
        "        if overlapping_speakers:\n",
        "\n",
        "            shortest_turn = min(overlapping_speakers, key=lambda x: x['duration'])\n",
        "            found_speaker = shortest_turn['speaker']\n",
        "        elif word_speakers and word['start'] - word_speakers[-1]['end'] < 0.2:\n",
        "            found_speaker = word_speakers[-1]['speaker']\n",
        "\n",
        "        word_speakers.append({\n",
        "            'word': word['word'],\n",
        "            'start': word['start'],\n",
        "            'end': word['end'],\n",
        "            'speaker': found_speaker if found_speaker else \"Unknown Speaker\"\n",
        "        })\n",
        "\n",
        "    dialogue_segments = []\n",
        "    if not word_speakers:\n",
        "        return \"\"\n",
        "\n",
        "    current_segment = {\n",
        "        'speaker': word_speakers[0]['speaker'],\n",
        "        'text': word_speakers[0]['word'],\n",
        "        'start': word_speakers[0]['start'],\n",
        "        'end': word_speakers[0]['end']\n",
        "    }\n",
        "\n",
        "    for i in range(1, len(word_speakers)):\n",
        "        word_info = word_speakers[i]\n",
        "        if word_info['speaker'] == current_segment['speaker']:\n",
        "            current_segment['text'] += word_info['word']\n",
        "            current_segment['end'] = word_info['end']\n",
        "        else:\n",
        "            dialogue_segments.append(current_segment)\n",
        "            current_segment = {\n",
        "                'speaker': word_info['speaker'],\n",
        "                'text': word_info['word'],\n",
        "                'start': word_info['start'],\n",
        "                'end': word_info['end']\n",
        "            }\n",
        "    dialogue_segments.append(current_segment)\n",
        "\n",
        "    srt_content = \"\"\n",
        "    idx = 1\n",
        "    for segment in dialogue_segments:\n",
        "        if segment['speaker'] == \"Unknown Speaker\":\n",
        "\n",
        "            continue\n",
        "\n",
        "        start_time_str = format_timestamp_srt(segment['start'])\n",
        "        end_time_str = format_timestamp_srt(segment['end'])\n",
        "        speaker_label = segment['speaker']\n",
        "        text = segment['text'].strip()\n",
        "\n",
        "        srt_content += f\"{idx}\\n\"\n",
        "        srt_content += f\"{start_time_str} --> {end_time_str}\\n\"\n",
        "        srt_content += f\"[{speaker_label}] {text}\\n\\n\"\n",
        "        idx += 1\n",
        "\n",
        "    return srt_content\n",
        "\n",
        "\n",
        "srt_output = generate_srt_from_results(whisper_result, diarization_result)\n",
        "\n",
        "print(\"--- Definitive SRT Content ---\")\n",
        "print(srt_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1PNa0513AA9",
        "outputId": "ff29fee3-f322-4c67-a56a-dd0f072c375c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Definitive SRT Content ---\n",
            "1\n",
            "00:00:01,100 --> 00:00:01,740\n",
            "[Speaker 1] I ask you a question?\n",
            "\n",
            "2\n",
            "00:00:02,860 --> 00:00:03,320\n",
            "[Speaker 1] course you can.\n",
            "\n",
            "3\n",
            "00:00:05,300 --> 00:00:06,460\n",
            "[Speaker 1] you get me out of this place?\n",
            "\n",
            "4\n",
            "00:00:07,520 --> 00:00:08,700\n",
            "[Speaker 1] afraid I'm of no use to you there.\n",
            "\n",
            "5\n",
            "00:00:09,020 --> 00:00:09,380\n",
            "[Speaker 2] Why\n",
            "\n",
            "6\n",
            "00:00:09,380 --> 00:00:15,860\n",
            "[Speaker 1] couldn't they put me in a proper prison? A better facility wasn't available. Or put me in a madhouse. It's a secure training centre. Training?\n",
            "\n",
            "7\n",
            "00:00:17,220 --> 00:00:18,360\n",
            "[Speaker 1] are they training us for?\n",
            "\n",
            "8\n",
            "00:00:19,220 --> 00:00:20,280\n",
            "[Speaker 1] other boys just scream all\n",
            "\n",
            "9\n",
            "00:00:20,280 --> 00:00:22,340\n",
            "[Speaker 2] the time. I thought this had been explained to you, Jamie. And they rock backwards and forwards\n",
            "\n",
            "10\n",
            "00:00:22,340 --> 00:00:23,060\n",
            "[Speaker 1] and they shower at current\n",
            "\n",
            "11\n",
            "00:00:23,060 --> 00:00:25,320\n",
            "[Speaker 2] agencies. Young offenders institutions have got 15\n",
            "\n",
            "12\n",
            "00:00:25,320 --> 00:00:26,920\n",
            "[Speaker 1] up. One kid goes nuts every time you\n",
            "\n",
            "13\n",
            "00:00:26,920 --> 00:00:29,500\n",
            "[Speaker 2] see Steve McDonald. It's a maddening day, you don't understand. This was\n",
            "\n",
            "14\n",
            "00:00:29,500 --> 00:00:30,580\n",
            "[Speaker 1] the best place for you\n",
            "\n",
            "15\n",
            "00:00:30,580 --> 00:00:31,100\n",
            "[Speaker 2] to be. Ryan\n",
            "\n",
            "16\n",
            "00:00:31,100 --> 00:00:35,000\n",
            "[Speaker 1] gets to stay at home until his trial. Ryan isn't accused of such a serious crime. Accused?\n",
            "\n",
            "17\n",
            "00:00:36,020 --> 00:00:36,480\n",
            "[Speaker 1] all accused.\n",
            "\n",
            "18\n",
            "00:00:37,500 --> 00:00:42,160\n",
            "[Speaker 1] I did it, if I hurt her, then I'd get it. But I didn't, so... I understand it, Jamie, and this wasn't my decision.\n",
            "\n",
            "19\n",
            "00:00:42,320 --> 00:00:43,800\n",
            "[Speaker 2] This was the best place for you to\n",
            "\n",
            "20\n",
            "00:00:43,800 --> 00:00:45,340\n",
            "[Speaker 1] be. Nothing has been decided yet.\n",
            "\n",
            "21\n",
            "00:00:45,400 --> 00:00:47,880\n",
            "[Speaker 2] You know where it's going to be, London or some local. You\n",
            "\n",
            "22\n",
            "00:00:47,880 --> 00:00:55,800\n",
            "[Speaker 1] need to sit down, Jamie. If you don't sit down... Does that mean you don't know what you just want to tell me? Nothing's been decided yet. You need to sit down. I\n",
            "\n",
            "23\n",
            "00:00:55,800 --> 00:00:56,880\n",
            "[Speaker 2] don't want to sit down. If you\n",
            "\n",
            "24\n",
            "00:00:56,880 --> 00:00:59,920\n",
            "[Speaker 1] don't sit down, I have to stop this discussion. Maybe. I don't want you to. Do you?\n",
            "\n",
            "25\n",
            "00:01:00,960 --> 00:01:01,860\n",
            "[Speaker 1] piss you off, that, wouldn't it?\n",
            "\n",
            "26\n",
            "00:01:02,540 --> 00:01:07,240\n",
            "[Speaker 1] have to go home and then come back again. Jamie, if you do not sit... I don't fucking want to sit down!\n",
            "\n",
            "27\n",
            "00:01:08,400 --> 00:01:10,280\n",
            "[Speaker 1] not tell me when to sit down!\n",
            "\n",
            "28\n",
            "00:01:11,280 --> 00:01:14,300\n",
            "[Speaker 1] do not control what I fucking... Look at me now!\n",
            "\n",
            "29\n",
            "00:01:16,040 --> 00:01:22,920\n",
            "[Speaker 1] do not control what I do in my life! Get that in that fucking little head of yours! Fucking hell!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further correct the SRT Gemini is used. Original audio is provided as the ground truth along with the diarization data and SRT data as flawed data. Gemini takes the SRT and diarization data as reference to frame the final \"output.srt\""
      ],
      "metadata": {
        "id": "zghwNweX5jBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "import os\n",
        "\n",
        "def correct_srt_with_gemini(api_key: str, audio_path: str, diarization_text: str, flawed_srt_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses Gemini to correct a flawed SRT file using audio and a diarization timeline.\n",
        "\n",
        "    Args:\n",
        "        audio_path: The file path to the audio to be analyzed.\n",
        "        diarization_text: A string containing the speaker diarization timeline to be used as a reference.\n",
        "        flawed_srt_text: A string containing the  SRT transcript to be used as a reference.\n",
        "\n",
        "    Returns:\n",
        "        The corrected SRT content as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "        print(\"Uploading audio file to Gemini...\")\n",
        "        audio_file = genai.upload_file(path=audio_path)\n",
        "        print(f\"âœ… Audio file '{audio_path}' uploaded successfully.\")\n",
        "\n",
        "\n",
        "        prompt = textwrap.dedent(\"\"\"\n",
        "        <prompt>\n",
        "    <system_instructions>\n",
        "        You are an expert AI subtitler and audio analyst.\n",
        "        Your primary task is to create a perfectly accurate, speaker-separated SRT subtitle file directly from the provided audio.\n",
        "\n",
        "        1.  **Analyze Audio Ground Truth:** The audio file is the single source of truth. Listen to it carefully to determine the exact words spoken, the timestamps, and the speakers.\n",
        "\n",
        "        2.  **Perform Your Own Speaker Diarization:** Identify the number of speakers in the audio yourself. Label them consistently as `[Speaker 1]`, `[Speaker 2]`, etc., based on the order of their first appearance.\n",
        "\n",
        "        3.  **Use Reference Data as Weak Hints:** You are also provided with two pieces of low-confidence reference data: a faulty diarization timeline and a flawed draft SRT. Use them only as weak hints if you are uncertain.\n",
        "\n",
        "        4.  **Generate a Perfect SRT File:** Your final output must be a single, complete SRT file.\n",
        "            -   The text, speaker labels, and timestamps must be derived directly and accurately from your analysis of the audio.\n",
        "            -   **Crucially, each distinct spoken utterance, even if it overlaps with another, must be its own numbered block with its own precise start and end times.**\n",
        "            -   Do not include any other text or explanations. Ensure the dialogue flows naturally and sentences are complete.\n",
        "\n",
        "        5.  **Required Output Format:**\n",
        "            ```srt\n",
        "            1\n",
        "            00:00:01,000 --> 00:00:04,000\n",
        "            [Speaker 1] Hello, welcome to the show.\n",
        "\n",
        "            2\n",
        "            00:00:04,100 --> 00:00:06,000\n",
        "            [Speaker 2] Thank you, it's great to be here.\n",
        "            ```\n",
        "    </system_instructions>\n",
        "\n",
        "    <reference_data>\n",
        "        <faulty_diarization_timeline>\n",
        "            {diarization_data}\n",
        "        </faulty_diarization_timeline>\n",
        "\n",
        "        <faulty_draft_srt>\n",
        "            {srt_data}\n",
        "        </faulty_draft_srt>\n",
        "    </reference_data>\n",
        "</prompt>\"\"\").format(diarization_data=diarization_text, srt_data=flawed_srt_text)\n",
        "\n",
        "\n",
        "        print(\"Sending request to Gemini for correction. This may take a moment...\")\n",
        "        model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
        "        response = model.generate_content([prompt, audio_file])\n",
        "        print(\"âœ… Received response from Gemini.\")\n",
        "\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    GEMINI_API_KEY = google_api_key\n",
        "    AUDIO_FILE_PATH = \"assignment_audio.wav\"\n",
        "\n",
        "    DIARIZATION_DATA = diarization_result\n",
        "    FLAWED_SRT_DATA = srt_output\n",
        "\n",
        "    if not os.path.exists(AUDIO_FILE_PATH):\n",
        "        print(f\"Error: Audio file not found at '{AUDIO_FILE_PATH}'\")\n",
        "        print(\"Please ensure the audio file is downloaded and the path is correct.\")\n",
        "    elif GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY\":\n",
        "        print(\"Error: Please replace 'YOUR_GEMINI_API_KEY' with your actual API key.\")\n",
        "    else:\n",
        "        corrected_srt = correct_srt_with_gemini(\n",
        "            api_key=GEMINI_API_KEY,\n",
        "            audio_path=AUDIO_FILE_PATH,\n",
        "            diarization_text=DIARIZATION_DATA,\n",
        "            flawed_srt_text=FLAWED_SRT_DATA\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- FINAL CORRECTED SRT (from Gemini) ---\")\n",
        "        print(corrected_srt)\n",
        "\n",
        "        with open(\"output.srt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(corrected_srt)\n",
        "            print(\"\\nâœ… Final corrected file saved as 'output.srt'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "82E6SNUsRwum",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "636efeea-e225-42e8-ff6c-daac31ec5c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading audio file to Gemini...\n",
            "âœ… Audio file 'assignment_audio.wav' uploaded successfully.\n",
            "Sending request to Gemini for correction. This may take a moment...\n",
            "âœ… Received response from Gemini.\n",
            "\n",
            "--- FINAL CORRECTED SRT (from Gemini) ---\n",
            "1\n",
            "00:00:00,996 --> 00:00:02,176\n",
            "[Speaker 1] Can I ask you a question?\n",
            "\n",
            "2\n",
            "00:00:02,896 --> 00:00:03,666\n",
            "[Speaker 2] Course you can.\n",
            "\n",
            "3\n",
            "00:00:05,256 --> 00:00:06,686\n",
            "[Speaker 1] Can you get me out of this place?\n",
            "\n",
            "4\n",
            "00:00:07,456 --> 00:00:08,896\n",
            "[Speaker 2] I'm afraid I'm of no use to you there.\n",
            "\n",
            "5\n",
            "00:00:09,056 --> 00:00:11,106\n",
            "[Speaker 1] Why can't they put me in a proper prison?\n",
            "\n",
            "6\n",
            "00:00:11,266 --> 00:00:12,746\n",
            "[Speaker 2] A better facility wasn't available.\n",
            "\n",
            "7\n",
            "00:00:13,116 --> 00:00:13,996\n",
            "[Speaker 1] Or put me in a madhouse.\n",
            "\n",
            "8\n",
            "00:00:14,246 --> 00:00:15,396\n",
            "[Speaker 2] It's a secure training centre.\n",
            "\n",
            "9\n",
            "00:00:15,706 --> 00:00:16,136\n",
            "[Speaker 1] Training?\n",
            "\n",
            "10\n",
            "00:00:17,216 --> 00:00:18,526\n",
            "[Speaker 1] What are they training us for?\n",
            "\n",
            "11\n",
            "00:00:19,266 --> 00:00:20,586\n",
            "[Speaker 1] The other boys just scream all the time.\n",
            "\n",
            "12\n",
            "00:00:20,226 --> 00:00:21,386\n",
            "[Speaker 2] I thought this had been explained to you, Jamie.\n",
            "\n",
            "13\n",
            "00:00:21,386 --> 00:00:25,566\n",
            "[Speaker 1] And they rock backwards and forwards, and they shout currents in, all the way from Boris.\n",
            "\n",
            "14\n",
            "00:00:23,546 --> 00:00:25,796\n",
            "[Speaker 2] Young Offenders Institutions have got 15 up.\n",
            "\n",
            "15\n",
            "00:00:25,976 --> 00:00:28,836\n",
            "[Speaker 1] One kid goes nuts every time you see Steve McDonald. It's maddening, you don't understand.\n",
            "\n",
            "16\n",
            "00:00:28,946 --> 00:00:30,686\n",
            "[Speaker 2] This was the best place for you to be.\n",
            "\n",
            "17\n",
            "00:00:31,066 --> 00:00:32,956\n",
            "[Speaker 1] Ryan gets to stay at home until his trial.\n",
            "\n",
            "18\n",
            "00:00:33,086 --> 00:00:34,806\n",
            "[Speaker 2] Ryan isn't accused of such a serious crime.\n",
            "\n",
            "19\n",
            "00:00:34,916 --> 00:00:35,506\n",
            "[Speaker 1] Accused?\n",
            "\n",
            "20\n",
            "00:00:36,126 --> 00:00:36,756\n",
            "[Speaker 1] It's all accused.\n",
            "\n",
            "21\n",
            "00:00:37,426 --> 00:00:40,296\n",
            "[Speaker 1] If I did it, if I hurt her, then I'd get it, but I didn't, so...\n",
            "\n",
            "22\n",
            "00:00:40,306 --> 00:00:42,266\n",
            "[Speaker 2] I understand it, Jamie, and this wasn't my decision.\n",
            "\n",
            "23\n",
            "00:00:42,506 --> 00:00:43,906\n",
            "[Speaker 1] Am I getting no for the trial?\n",
            "\n",
            "24\n",
            "00:00:42,676 --> 00:00:43,986\n",
            "[Speaker 2] This was the best place for you to be.\n",
            "\n",
            "25\n",
            "00:00:44,306 --> 00:00:45,456\n",
            "[Speaker 2] Nothing has been decided yet.\n",
            "\n",
            "26\n",
            "00:00:45,556 --> 00:00:47,746\n",
            "[Speaker 1] Do you know where it's gonna be, London or some other local?\n",
            "\n",
            "27\n",
            "00:00:47,746 --> 00:00:49,706\n",
            "[Speaker 2] You need to sit down, Jamie.\n",
            "\n",
            "28\n",
            "00:00:50,006 --> 00:00:51,126\n",
            "[Speaker 2] If you don't sit down, I...\n",
            "\n",
            "29\n",
            "00:00:51,336 --> 00:00:53,496\n",
            "[Speaker 1] Does that mean you don't know? Does that mean you don't know what you just want to tell me?\n",
            "\n",
            "30\n",
            "00:00:53,626 --> 00:00:55,756\n",
            "[Speaker 2] Nothing's been decided yet. You need to sit down.\n",
            "\n",
            "31\n",
            "00:00:56,066 --> 00:00:56,876\n",
            "[Speaker 1] I don't wanna sit down!\n",
            "\n",
            "32\n",
            "00:00:56,926 --> 00:00:58,746\n",
            "[Speaker 2] If you don't sit down, I have to stop this discussion.\n",
            "\n",
            "33\n",
            "00:00:58,896 --> 00:00:59,696\n",
            "[Speaker 1] Maybe I want you to.\n",
            "\n",
            "34\n",
            "00:00:59,796 --> 00:01:00,246\n",
            "[Speaker 2] Do you?\n",
            "\n",
            "35\n",
            "00:01:01,086 --> 00:01:02,286\n",
            "[Speaker 1] It'd piss you off that, wouldn't it?\n",
            "\n",
            "36\n",
            "00:01:02,776 --> 00:01:04,176\n",
            "[Speaker 1] You'd have to go home and then come back again.\n",
            "\n",
            "37\n",
            "00:01:04,396 --> 00:01:05,536\n",
            "[Speaker 2] Jamie, if you do not sit...\n",
            "\n",
            "38\n",
            "00:01:05,536 --> 00:01:07,176\n",
            "[Speaker 1] I don't fucking want to sit down!\n",
            "\n",
            "39\n",
            "00:01:08,446 --> 00:01:10,346\n",
            "[Speaker 1] You do not tell me when to sit down!\n",
            "\n",
            "40\n",
            "00:01:11,156 --> 00:01:14,486\n",
            "[Speaker 1] You do not control what I fucking! Look at me now!\n",
            "\n",
            "41\n",
            "00:01:15,866 --> 00:01:20,536\n",
            "[Speaker 1] You do not control what I do in my life! Get that in that fucking little head of yours!\n",
            "\n",
            "42\n",
            "00:01:22,126 --> 00:01:22,866\n",
            "[Speaker 1] Fucking hell!\n",
            "\n",
            "âœ… Final corrected file saved as 'output_gemini_corrected.srt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QC Agent\n",
        "\n",
        "Provided both the visual approach (Human in loop)\n",
        "and\n",
        "Agentic approach"
      ],
      "metadata": {
        "id": "Peo8vbs9W7yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for the visual testing of the \"FINAL PRODUCT\" just run and watch the entire video with subtitles. \"final_video_with_subtitles.mp4\""
      ],
      "metadata": {
        "id": "ioaYw6Wj7SWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def burn_subtitles_with_audio(video_path: str, srt_path: str, output_path=\"final_video_with_subtitles.mp4\"):\n",
        "    \"\"\"\n",
        "    Burns the SRT subtitles into the video while copying the original audio stream.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video file.\n",
        "        srt_path (str): Path to the SRT subtitle file.\n",
        "        output_path (str): Path for the final output video.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Merging Subtitles and Audio into Video ---\")\n",
        "\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"âŒ Error: Input video not found at '{video_path}'\")\n",
        "        return\n",
        "    if not os.path.exists(srt_path):\n",
        "        print(f\"âŒ Error: Subtitle file not found at '{srt_path}'\")\n",
        "        return\n",
        "\n",
        "\n",
        "    command = f\"\"\"\n",
        "    ffmpeg -i \"{video_path}\" -vf \"subtitles='{srt_path}':force_style='Fontsize=20,PrimaryColour=&H00FFFFFF&,BorderStyle=3'\" -c:a copy \"{output_path}\" -y\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸ”¥ Burning subtitles into video...\")\n",
        "\n",
        "    os.system(command)\n",
        "\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"âœ… Final video created: '{output_path}'\")\n",
        "    else:\n",
        "        print(f\"âŒ Failed to create final video.\")\n",
        "\n",
        "video_output_path = \"/content/assignment_video.mp4\"\n",
        "final_srt_file = \"/content/output.srt\"\n",
        "\n",
        "\n",
        "burn_subtitles_with_audio(video_output_path, final_srt_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnPaVnIIUnl0",
        "outputId": "6feeeae9-46df-406c-ff9f-2b8afd3645fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Merging Subtitles and Audio into Video ---\n",
            "ðŸ”¥ Burning subtitles into video...\n",
            "âœ… Final video created: 'final_video_with_subtitles.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used as Quality Check Agent, Gemini is given the \"original audio\" as ground truth and the \"output.srt\" as the test. with the given task:\n",
        "1.  Verify the accuracy of the transcribed text.\n",
        "2.  Verify the accuracy of the assigned speaker label.\n",
        "3.  Verify the accuracy of the start and end timestamps.\n",
        "\n",
        "To provide both confidence score with feedback.\n",
        "\n",
        "which is saved as \"quality_report.csv\""
      ],
      "metadata": {
        "id": "6K8Al_Az7-J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import pandas as pd\n",
        "\n",
        "client = genai.Client(api_key=google_api_key)\n",
        "\n",
        "myfile = client.files.upload(file=\"/content/assignment_audio.wav\")\n",
        "print(\"Original Audio uploaded !\")\n",
        "\n",
        "prompt = f\"\"\"<prompt>\n",
        "    <system_instructions>\n",
        "        You are a meticulous Quality Assurance (QA) Analyst for Speaker Diarization. Your task is to analyze each individual subtitle chunk in the provided SRT file against the video ground truth (provided audio).\n",
        "        There are multiple speakers speaking in the audio thus, ensure the SRT file justifies the speaker diarization timeline.\n",
        "    </system_instructions>\n",
        "\n",
        "    <input_data>\n",
        "        <full_subtitle_file_to_evaluate>\n",
        "            {corrected_srt}\n",
        "        </full_subtitle_file_to_evaluate>\n",
        "    </input_data>\n",
        "\n",
        "    <task>\n",
        "        For **every numbered chunk** in the subtitle file, perform the following analysis by listening to the audio at the corresponding timestamp:\n",
        "        1.  Verify the accuracy of the transcribed text.\n",
        "        2.  Verify the accuracy of the assigned speaker label.\n",
        "        3.  Verify the accuracy of the start and end timestamps.\n",
        "    </task>\n",
        "\n",
        "    <output_format>\n",
        "        Your response MUST be a raw CSV string and nothing else. Do not include any other text, explanations, or markdown formatting.\n",
        "\n",
        "        The CSV string must have the following header row:\n",
        "        `index,confidence_score,feedback`\n",
        "\n",
        "        Each subsequent row must correspond to a subtitle chunk from the input file. Ensure that any feedback containing commas is enclosed in double quotes.\n",
        "\n",
        "        Example:\n",
        "        ```csv\n",
        "        index,confidence_score,feedback\n",
        "        1,10,\"Perfect match in text, speaker, and timing.\"\n",
        "        2,7,\"The text is accurate, but the subtitle starts slightly late.\"\n",
        "        3,4,\"The speaker label is incorrect; Speaker 2's voice is heard.\"\n",
        "        ```\n",
        "    </output_format>\n",
        "</prompt>\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=[prompt, myfile]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Quality Check Report (CSV) ---\")\n",
        "\n",
        "csv_response_text = response.text.strip().replace(\"```csv\", \"\").replace(\"```\", \"\")\n",
        "file_path = \"quality_report.csv\"\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(csv_response_text.strip())\n",
        "\n",
        "print(f\"âœ… Report successfully saved to '{file_path}'\")\n",
        "\n",
        "print(\"\\n--- Quality Check Report ---\")\n",
        "report_df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "print(report_df.to_string())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wThL9EsImVnT",
        "outputId": "d87bbc8e-f2cd-4493-eb8c-0c82f5c5bb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Audio uploaded !\n",
            "\n",
            "--- Quality Check Report (CSV) ---\n",
            "âœ… Report successfully saved to 'quality_report.csv'\n",
            "\n",
            "--- Quality Check Report ---\n",
            "    index  confidence_score                                                                                                                                                  feedback\n",
            "0       1                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "1       2                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "2       3                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "3       4                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "4       5                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "5       6                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "6       7                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "7       8                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "8       9                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "9      10                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "10     11                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "11     12                 7  The start timestamp is too early, causing an artificial overlap with Speaker 1's previous utterance. Speaker 2 begins speaking after Speaker 1 finishes.\n",
            "12     13                 4                                        Transcription is incorrect; 'currents in' should be 'Corin, Corin is in!'. The end timestamp is slightly too late.\n",
            "13     14                10                                                                Perfect match in text, speaker, and timing, accurately reflecting the intentional overlap.\n",
            "14     15                10                                                                Perfect match in text, speaker, and timing, accurately reflecting the intentional overlap.\n",
            "15     16                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "16     17                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "17     18                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "18     19                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "19     20                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "20     21                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "21     22                10                                                                Perfect match in text, speaker, and timing, accurately reflecting the intentional overlap.\n",
            "22     23                 7                                                                                                               Transcription error: 'no' should be 'know'.\n",
            "23     24                10                                                                Perfect match in text, speaker, and timing, accurately reflecting the intentional overlap.\n",
            "24     25                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "25     26                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "26     27                 7                                                       The start timestamp is slightly early, causing a minor overlap with Speaker 1's previous utterance.\n",
            "27     28                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "28     29                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "29     30                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "30     31                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "31     32                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "32     33                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "33     34                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "34     35                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "35     36                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "36     37                10                                                                 Perfect match in text, speaker, and timing, accurately reflecting the intentional cutoff.\n",
            "37     38                10                                                                Perfect match in text, speaker, and timing, accurately reflecting the intentional overlap.\n",
            "38     39                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "39     40                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "40     41                10                                                                                                               Perfect match in text, speaker, and timing.\n",
            "41     42                10                                                                                                               Perfect match in text, speaker, and timing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Sarika\n",
        "\n",
        "Planned to use Sarika for generate another confidence score by comparing between diarization of Sarika and My agentic output.\n",
        "But Couldn't do it since i was not able to find any documentation to perform diarization with Saarika-v2.5 (as mentioned in the provided docs)"
      ],
      "metadata": {
        "id": "_MVVk1msqkuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_initial_audio(video_path: str, duration_seconds: int, output_audio_path: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts a specific duration of audio from the beginning of a video file.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the input video file.\n",
        "        duration_seconds (int): The duration to extract in seconds.\n",
        "        output_audio_path (str): The path to save the extracted WAV audio file.\n",
        "\n",
        "    Returns:\n",
        "        The path to the output audio file, or None if an error occurred.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"Input video not found at: {video_path}\")\n",
        "\n",
        "        print(f\"ðŸŽ§ Loading video: '{video_path}'\")\n",
        "        full_audio = AudioSegment.from_file(video_path)\n",
        "\n",
        "        # Slice the audio to 25 sec (limit is 30 without batch)\n",
        "        duration_ms = duration_seconds * 1000\n",
        "        initial_audio_segment = full_audio[:duration_ms]\n",
        "\n",
        "\n",
        "        initial_audio_segment.export(output_audio_path, format=\"wav\")\n",
        "        print(f\"âœ… Successfully extracted {duration_seconds}s of audio to '{output_audio_path}'\")\n",
        "        return output_audio_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "extracted_file = extract_initial_audio(\n",
        "        video_path=\"/content/assignment_video.mp4\",\n",
        "        duration_seconds=25,\n",
        "        output_audio_path=\"/content/sarika_audio.wav\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_bkRXGXXEJ6",
        "outputId": "a234879a-8dad-490f-8546-7f66d017d6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ§ Loading video: '/content/assignment_video.mp4'\n",
            "âœ… Successfully extracted 25s of audio to '/content/sarika_audio.wav'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sarvamai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYnobSj2Xop8",
        "outputId": "446b2ce9-f524-4f25-e152-b24b778743d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sarvamai\n",
            "  Downloading sarvamai-0.1.15-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from sarvamai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from sarvamai) (2.11.7)\n",
            "Requirement already satisfied: pydantic-core>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from sarvamai) (2.33.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from sarvamai) (4.14.1)\n",
            "Requirement already satisfied: websockets>=12.0 in /usr/local/lib/python3.11/dist-packages (from sarvamai) (15.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->sarvamai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->sarvamai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->sarvamai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->sarvamai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->sarvamai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->sarvamai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->sarvamai) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->sarvamai) (1.3.1)\n",
            "Downloading sarvamai-0.1.15-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sarvamai\n",
            "Successfully installed sarvamai-0.1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sarvamai import SarvamAI\n",
        "\n",
        "client = SarvamAI(\n",
        "    api_subscription_key= sarvamai_key\n",
        ")\n",
        "\n",
        "response = client.speech_to_text.transcribe(\n",
        "    file=open(\"/content/sarika_audio.wav\", \"rb\"),\n",
        "    model=\"saarika:v2.5\",\n",
        "    language_code=\"en-IN\",\n",
        "    #enable_diarization=True  #couldn't found the parameter to enable diarization\n",
        "    #diarized_transcript=True\n",
        ")\n",
        "\n",
        "print(response.transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uemLBlfLXiea",
        "outputId": "98caf63a-de5c-4821-a304-8524afa7f1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can I ask you a question? Of course you can.  Can you can you get me out of this place? I'm afraid I'm of no use to you there. Why why couldn't they put me in a proper prison? A better facility wasn't available. Or put me in a madhouse. It's a secure training center. Training? Well what are they training us for? The other boys just scream all the time. And they walk practicing forward and they shall walk coming in. Young offenders institutions are always a problem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-Is2ztU3QsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}